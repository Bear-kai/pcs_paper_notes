# Object Pose Estimation

## self-add

- **RNNPose** Yan Xu (CUHK), RNNPose: Recurrent 6-DoF Object Pose Refinement with Robust Correspondence Field Estimation and Pose Optimization. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_RNNPose_Recurrent_6-DoF_Object_Pose_Refinement_With_Robust_Correspondence_Field_CVPR_2022_paper.pdf)] [[github](https://github.com/DecaYale/RNNPose)] [cite 2]

- **GPV-Pose** Yan Di (慕尼黑工大), GPV-Pose: Category-level Object Pose Estimation via Geometry-guided Point-wise Voting [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Di_GPV-Pose_Category-Level_Object_Pose_Estimation_via_Geometry-Guided_Point-Wise_Voting_CVPR_2022_paper.pdf)] [[github](https://github.com/lolrudy/GPV_Pose)] [cite ]


- **Gen6D** Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images [[arxiv 2022](https://arxiv.org/pdf/2204.10776.pdf)] [[github](https://liuyuan-pal.github.io/Gen6D/)]

- **OSOP** Ivan Shugurov (慕尼黑工大), OSOP: A Multi-Stage One Shot Object Pose Estimation Framework [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Shugurov_OSOP_A_Multi-Stage_One_Shot_Object_Pose_Estimation_Framework_CVPR_2022_paper.pdf)] 

- **ZebraPose** Yongzhi Su (DFKI), ZebraPose: Coarse to Fine Surface Encoding for 6DoF Object Pose Estimation. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Su_ZebraPose_Coarse_To_Fine_Surface_Encoding_for_6DoF_Object_Pose_CVPR_2022_paper.pdf)] [[github]( https://github.com/suyz526/ZebraPose)] [cite 1]

- **FS6D** Yisheng He (HKUST & 旷视), FS6D: Few-Shot 6D Pose Estimation of Novel Objects. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/He_FS6D_Few-Shot_6D_Pose_Estimation_of_Novel_Objects_CVPR_2022_paper.pdf)] [[github](https://fs6d.github.io/)] [cite 1]

- **UDA-COPE** Taeyeop Lee (KAIST), UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_UDA-COPE_Unsupervised_Domain_Adaptation_for_Category-Level_Object_Pose_Estimation_CVPR_2022_paper.pdf)] [cite 1]

- **SurfEmb** SurfEmb: Dense and Continuous Correspondence Distributions
for Object Pose Estimation with Learnt Surface Embeddings. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Haugaard_SurfEmb_Dense_and_Continuous_Correspondence_Distributions_for_Object_Pose_Estimation_CVPR_2022_paper.pdf)] [[github](https://surfemb.github.io/)] [cite 3]

- **TemplatePose** Van Nguyen Nguyen (CNRS, France), Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Nguyen_Templates_for_3D_Object_Pose_Estimation_Revisited_Generalization_to_New_CVPR_2022_paper.pdf)] [[github](https://github.com/nv-nguyen/template-pose)] [cite 2]

- **DualPoseNet** Jiehong Lin,...,Kui Jia, (华南理工 & 华为), DualPoseNet: Category-level 6D Object Pose and Size Estimation
Using Dual Pose Network with Refined Learning of Pose Consistency. [[ICCV 2021](http://openaccess.thecvf.com/content/ICCV2021/papers/Lin_DualPoseNet_Category-Level_6D_Object_Pose_and_Size_Estimation_Using_Dual_ICCV_2021_paper.pdf)] [[github]()] [cite 17]


- **KDFNet** Xingyu Liu (CMU), KDFNet: Learning Keypoint Distance Field for 6D Object Pose Estimation. [[IROS 2021](https://arxiv.org/pdf/2109.10127)] [cite 1]

- **CenterSnap** Muhammad Zubair Irshad (乔治亚理工 & 丰田研究院), CenterSnap: Single-Shot Multi-Object 3D Shape Reconstruction and Categorical 6D Pose and Size Estimation. [[ICRA 2022](https://arxiv.org/pdf/2203.01929)] [[github](https://github.com/zubair-irshad/CenterSnap)] [cite 2]

- **YOLOPose** YOLOPose: Transformer-based Multi-Object 6D Pose Estimation using Keypoint Regression. [[arxiv 2022](https://arxiv.org/pdf/2205.02536)]


- **OnePose** Jiaming Sun (浙大&商汤), OnePose: One-Shot Object Pose Estimation without CAD Models. [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_OnePose_One-Shot_Object_Pose_Estimation_Without_CAD_Models_CVPR_2022_paper.pdf)] [[github](https://zju3dv.github.io/onepose/)] 


- **`TemporalFusion`** Fengjun Mu (中科大), TemporalFusion: Temporal Motion Reasoning with Multi-Frame Fusion for 6D Object Pose Estimation. [[IROS 2021](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9636583)] [[github](https://github.com/mufengjun260/TemporalFusion21)] [cite 0]

- **`TemporalFusion扩展`** Rui Huang (中科大), Estimating 6D Object Poses with Temporal Motion Reasoning for Robot Grasping in Cluttered Scenes. [[RAL 2022](https://ieeexplore.ieee.org/abstract/document/9699040/)] [[github](https://github.com/mufengjun260/H-MPose)] [cite 0]


- **Morefusion** Kentaro Wada (帝国理工), Morefusion: Multi-object reasoning for 6d pose estimation from volumetric fusion, [[CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wada_MoreFusion_Multi-object_Reasoning_for_6D_Pose_Estimation_from_Volumetric_Fusion_CVPR_2020_paper.pdf)] [[github]()] [cite 48]


- **Cosypose** Cosypose: Consistent multi-view multi-object 6d pose estimation. [[ECCV 2020](https://arxiv.org/pdf/2008.08465)] [[page](https://www.di.ens.fr/willow/research/cosypose/)] [cite 133]


- **Latentfusion** Latentfusion: End-to-end differentiable reconstruction and rendering for unseen object pose estimation. [[CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Park_LatentFusion_End-to-End_Differentiable_Reconstruction_and_Rendering_for_Unseen_Object_Pose_CVPR_2020_paper.pdf)] [[github](https://keunhong.com/publications/latentfusion/)] [cite 54]


- TCLCH, Real-Time Monocular Pose Estimation of 3D Objects using
Temporally Consistent Local Color Histograms. [[ICCV 2017](https://openaccess.thecvf.com/content_ICCV_2017/papers/Tjaden_Real-Time_Monocular_Pose_ICCV_2017_paper.pdf)] [cite 90]
（RBOT的三篇论文之一）

- Neural Object Fitting - Category level object pose estimation via neural analysis-by-synthesis. [[ECCV 2020](https://arxiv.org/pdf/2008.08145)] [cite 46]

- **RLLG** Ming Cai (阿德莱德大学), Reconstruct locally, localize globally: A model free method for object pose estimation. [[CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Cai_Reconstruct_Locally_Localize_Globally_A_Model_Free_Method_for_Object_CVPR_2020_paper.pdf)] [cite 9]

- Objectron: A large scale dataset of object-centric videos in the wild with pose
annotations. [[CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Ahmadyan_Objectron_A_Large_Scale_Dataset_of_Object-Centric_Videos_in_the_CVPR_2021_paper.pdf)] [cite 45]


- **SGPA**: Kai Chen, Qi Dou (CUHK), Structure-Guided Prior Adaptation for
Category-Level 6D Object Pose Estimation. [[ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_SGPA_Structure-Guided_Prior_Adaptation_for_Category-Level_6D_Object_Pose_Estimation_ICCV_2021_paper.pdf)] [[page](https://www.cse.cuhk.edu.hk/˜kaichen/projects/sgpa/sgpa.html)] [cite 14]

- **PPR-Net** Zhikai Dong (商汤&清华), PPR-Net:Point-wise Pose Regression Network for Instance Segmentation and 6D Pose Estimation in Bin-picking Scenarios. [[IROS 2019](https://ieeexplore.ieee.org/abstract/document/8967895/)] [[github](https://github.com/lvwj19/PPR-Net-plus)] [cite 26]

## Others

- **UniPose** Bruno Artacho (RIT), UniPose: Unified Human Pose Estimation in Single Images and Videos. [[CVPR 2020](https://openaccess.thecvf.com/content_CVPR_2020/papers/Artacho_UniPose_Unified_Human_Pose_Estimation_in_Single_Images_and_Videos_CVPR_2020_paper.pdf)] [cite 78]


- **MaskFusion** Martin Runz (伦敦学院大学), MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects. [[ISMAR 2018](https://arxiv.org/pdf/1804.09194)] [[page](http://visual.cs.ucl.ac.uk/pubs/maskfusion/)] [cite 207]
    
    物体级的语义动态SLAM-也许能用到它的物体3D重构。


- **--** [[CVPR ]()] [[github]()] [cite ]



- - -

## Paper notes


<details>
<summary> <b> RNNPose (CVPR 2022) - 关注 </b> </summary>

- 需要提供物体的CAD模型，和初始的pose；注意它的目的是做pose refinement!
- 对我而言，它的优点是较好地将RAFT框架和pose微调任务结合起来了，并且利用了render和非线性优化技术得到end2end模型，以及明确了光流场和3D刚体变换之间的关系(Eq.1)；缺点是，如作者本人所言，该模型是object-specific，对于novel object, pose refinement module需要进一步被微调！

- 摘要：本文提出一种方法，从**单目图像**中估计物体的6-Dof位姿，采用了基于RNN的框架，能较鲁棒地应对erroneous初始pose和遮挡问题。在循环迭代中，基于估计的匹配场（correspondence field），物体的pose优化被建模为非线性最小二乘问题，然后基于可微的LM优化算法求解，可实现端到端训练，其中，匹配场的估计和位姿优化这两个步骤是交替进行的。在LINEMOD，Occlusion-LINEMOD和YCB-Video上达到了SOTA效果。

- 算法架构：3D model表示成从各个角度渲染的2D query template的集合。
    ![RNNPose_archi](assets_pose/RNNPose_archi.png)

- 详细框架：其中render是基于pytorch3d；render包括将3D model根据init_pose渲染为图片，还有将3D features渲染为2D context feature map。
![RNNPose_fig2](assets_pose/RNNPose_fig2.png)

<summary>
</details>


<details>
<summary> <b> GPV-Pose (CVPR 2022) - 关注 </b> </summary>

- 摘要：利用几何信息来增强类别级pose估计的特征学习，一者引入解耦的置信度驱动的旋转表达，二来提出几何引导的逐点投票进行3D bbox估计。最后，利用不同的输出流，加入几何一致性约束，可以进一步提升性能。GPV-Pose的推理速度能达到20FPS。

- 网络结构：下面$r_x$和$r_y$是平面法向；预测的残差平移，通过加上输入点云的均值，得到最终的平移量；预测的残差size，通过加上预先计算的类均值size，得到最终的size；对称考虑镜像对称和旋转对称；对于逐点bbox投票，给每个点预测它到每个面的方向，距离和置信度，因此每个点要预测的维度就是$(3+1+1)\times 6 = 30$；置信度感知的损失函数（论文中Eq.(1)和Eq.(6)有点意思）；关于2个几何约束，暂略。
    ![GPV_archi](assets_pose/GPV_archi.png)

<summary>
</details>


<details>
<summary> <b> Gen6D (arxiv 2022) </b> </summary>

- 摘要：Existing generalizable pose estimators either need the high-quality object models or require additional depth maps or object masks in test time, which significantly limits their
application scope. In contrast, our pose estimator only requires some posed images of the unseen object and is able to accurately predict poses of the object in arbitrary environments. Gen6D consists of an object detector, a viewpoint selector and a pose refiner, all of which do not require
the 3D object model and can generalize to unseen objects. 

- 作者argue一个pose估计器应该具有的性质包括：1) 泛化性，指泛化到任何物体；2) model-free；3) simple-inputs,只须输入rgb，无需物体mask或depth map。考虑到基于回归的旋转和平移预测，受限于特定的实例或者类别，无法泛化到任意未见物体；并且，由于缺乏3D model，无法构建2D-3D匹配，因此基于PnP的方法也不能用，因此作者采用基于图像匹配的方式，进行coarse-to-fine的pose估计。

- 网络结构：模型输入是一张query image，和一堆reference image，并且参考图像中的物体pose是已知的，感觉这个要求也不实用啊？！文中说的是："Given Nr images of an object with known camera poses"，搞不清到底是已知谁的pose；另外Data normalization中提到利用三角化来估计物体的size，存疑，三角化不是存在尺度不确定性问题嚒？！

- 大致流程：基于correlation的object location，定位出query图片中的物体位置，得出大致的平移分量；然后基于网络学习相似性度量，挑选ref图像中view最接近的图像，结合预测的in-plane rotation得粗糙的旋转分量；最后pose refiner利用3D CNN和transformer的特征信息融合方式，进行输出微调。

- **这个应该是Few-shot setting，相当于通过少量标注样本，就可以泛化到该instance，另参FS6D**。虽然无须3DCAD model, depth和mask信息，但要提供一些ref图片！

    ![Gen6D_vis](assets_pose/Gen6D_vis.png)
    ![Gen6D_archi](assets_pose/Gen6D_archi.png)

<summary>
</details>


<details>
<summary> <b> OSOP (CVPR 2022) </b> </summary>

- 2D-2D匹配 + 2D-3D匹配（PnP with RANSAC）
- 摘要：We present a novel one-shot method for object detection
and 6 DoF pose estimation, that does not require training
on target objects. At test time, it takes as input a target image and a textured 3D query model. The core idea is to represent a 3D model with a number of 2D templates rendered from different viewpoints. This enables CNN-based
direct dense feature extraction and matching. The object is
first localized in 2D, then its approximate viewpoint is estimated, followed by dense 2D-3D correspondence prediction. The final pose is computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed, YCB-V and TLESS datasets.

- 网络结构
    ![OSOP_pipe](assets_pose/OSOP_pipe.png)

<summary>
</details>


<details>
<summary> <b> ZebraPose (CVPR 2022) </b> </summary>

- 摘要：we present a discrete descriptor, which can represent the object surface
densely. By incorporating a hierarchical binary grouping, we can encode the object surface very efficiently. Moreover,
we propose a coarse to fine training strategy, which enables fine-grained correspondence prediction. Finally, by matching predicted codes with object surface and using a PnP solver, we estimate the 6DoF pose. **In summary, we propose ZebraPose, a two-stage RGBbased approach that defines the matching of dense 2D-3D correspondence as a hierarchical classification task**.

- Surface encoding：以简单情形为例：编码长度为$d$，即对object的顶点进行$d$次分组（提到用kmeans），每一次迭代分组，每个顶点都被赋一个类别id（binary取值），最后把$d$个类id堆叠起来就是顶点的code，可知一个group内的顶点共享code。对于每个3D model，都建立这样的表达并存储起来。

- 网络结构：自大致理解，基于网络预测像素的code，然后跟3D model预先建立的顶点code进行比较，即可构建2D-3D的匹配关系，然后PnP求解pose。
    ![ZebraPose_vis](assets_pose/ZebraPose_vis.png)
    ![ZebraPose_archi](assets_pose/ZebraPose_archi.png)

<summary>
</details>


<details>
<summary> <b> FS6D (CVPR 2022) </b> </summary>

- 摘要：We study a new open set problem; the few-shot 6D object poses estimation: estimating the 6D pose of an unknown object by a few support views without extra training. We propose a dense prototypes matching framework by extracting and matching dense RGBD prototypes with transformers. We propose a large-scale RGBD photorealistic dataset (ShapeNet6D) for network pre-training.

- 网络结构
    ![FS6D_data](assets_pose/FS6D_data.png)
    ![FS6D_pipe](assets_pose/FS6D_pipe.png)

<summary>
</details>


<details>
<summary> <b> UDA-COPE (CVPR 2022) </b> </summary>

- 是第一个对基于RGBD的类级别物体姿态估计做无监督域适应的工作；
- 摘要：The proposed method exploits
a teacher-student self-supervised learning scheme to train a pose estimation network without using target domain pose labels. We also introduce a bidirectional filtering method between the predicted normalized object coordinate space (NOCS) map and observed point cloud, to not only make
our teacher network more robust to the target domain but also to provide more reliable pseudo labels for the student network training.

- 网络结构：(1) 先在合成数据上对教师网络进行有监督训练，再在真实数据上进行无监督域适应；(2) Fig.1是Fig.2中model的网络结构，注意2D特征是与点云有效匹配的特征，是从特征图中采样（4.1节提到把图像块resize为192x192的大小，再随机采样1024个点，应该是指对应这些点的2D特征）；(3) 为了让合成数据上训练的教师网络，在真实数据上的预测更好，即学生网络的伪标签更好，提出Fig.3所示的双向点滤波，看上去比较简单，就是基于老师网络的初始预测，将depth点云对齐到NOCS map，然后计算对齐后二者的逐点距离，设定阈值，分别从两个方向上过滤掉距离值大的异常点；(4) 联合训练老师网络和学生网络，具体地，利用过滤后的NOCS图作为学生网络的伪标签；同时，老师网络基于自监督开始学习真实数据上的知识，作者提出利用几何一致性，通过交叉熵损失，约束过滤后的对齐点云（可能只是原始点云的一个很小子集）要和老师网络自己预测的NOCS图一致！
    ![UDA_archi](assets_pose/UDA_archi.png)
    ![UDA_vis](assets_pose/UDA_vis.png)

<summary>
</details>


<details>
<summary> <b> TemplatePose (CVPR 2022) </b> </summary>

- 关键词：Model-based；图像匹配；泛化到长的很不一样的物体上。
- 摘要：Our method requires neither a training phase on these objects nor real images depicting them, only their CAD models. It relies on a small set of training objects to learn local object representations, which allow us to locally match the input image to a set of “templates”, rendered images of the CAD models for the new objects. As a result, we are the first to show generalization without retraining on the LINEMOD and
Occlusion-LINEMOD datasets.

- 网络结构
    ![TemplatePose_vis](assets_pose/TemplatePose_vis.png)
    ![TemplatePose_archi](assets_pose/TemplatePose_archi.png)

<summary>
</details>


<details>
<summary> <b> SurfEmb (CVPR 2022) </b> </summary>

- 关键词：基于对比学习，构建2D-3D稠密匹配。

- 摘要：We present an approach to learn dense, continuous 2D-3D correspondence distributions over the surface of objects. We also present a new method for 6D pose estimation of rigid objects using the learnt distributions to sample, score and refine pose hypotheses. The correspondence distributions are learnt with a contrastive loss.

- Overview: We base our method on image crops from a detection model. We feed an image crop through our model to obtain dense (pixelwise) surface distributions and a mask which together form a correspondence distribution. In a PnP-RANSAC fashion, we sample pose hypotheses from the correspondence distribution and score each hypothesis based on the mask and surface distributions. The best scoring pose hypothesis is then refined based on the surface distributions to obtain the final pose estimate.

- 算法流程：四阶段方法：检测物体，从crop图中学习分布，从分布中得到初始pose，refinement。
    ![SurfEmb_vis](assets_pose/SurfEmb_vis.png)

<summary>
</details>


<details>
<summary> <b> DualPoseNet (ICCV 2021) </b> </summary>

- 核心：同时使用“直接pose回归”和“NOCS坐标预测”两种pose估计方案。

- 摘要：DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder. The implicit and explicit decoders thus impose complementary supervision on the training of pose encoder. We construct the encoder
based on spherical convolutions, and design Spherical Fusion wherein for a better embedding of pose sensitive features from the appearance and shape observations.

- 网络结构
    ![DualPoseNet_archi](assets_pose/DualPoseNet_archi.png)

- Refinement：在测试阶段，共有3种方式可以得到pose参数：(1)直接从explicit decoder的结果中得到pose参数；(2)从implicit decoder中得到NOCS坐标预测，然后Umeyama算法求解pose参数；(3) refinement。这里介绍第三种refine的方式，它是利用2个decoder的预测结果的几何一致性作为Loss，即输入点云P经过显式预测的R|T|s变换后得到的标准坐标，要和隐式预测的标准坐标一致，在测试阶段，固定两个decoder网络参数，并单独优化encoder。
    ![DualPoseNet_refine](assets_pose/DualPoseNet_refine.png)

<summary>
</details>


<details>
<summary> <b> KDFNet (IROS 2021) </b> </summary>

- 关键词：Model-based；预测关键点；PnP
- 要解决：基于pixel-wise voting的方法是direction-based，即每个像素预测它到关键点的2D方向；该方法有一个前提假设，投票方向之间的夹角要足够大，因此该假设不适用于细长的物体，为此，本文提出KDF。
- 摘要：We propose a novel continuous representation called Keypoint Distance Field
(KDF) for projected 2D keypoint locations. Formulated as a 2D array, each element of the KDF stores the 2D Euclidean distance between the corresponding image pixel and a specified
projected 2D keypoint. We use a fully convolutional neural network to regress the KDF for each keypoint.

- 网络结构
    ![KDFNet_vis](assets_pose/KDFNet_vis.png)
    ![KDFNet_archi](assets_pose/KDFNet_archi.png)

<summary>
</details>


<details>
<summary> <b> CenterSnap (ICRA 2022) - 关注(重建+pose) </b> </summary>

- 要解决：现有的基于“标准坐标回归”和“直接pose回归”方案，计算量大，并且在复杂的多目标场景中性能不好。Existing approaches mainly follow a complex multi-stage pipeline which first localizes and detects each object instance in the image and then regresses to either their 3D meshes or 6D poses. These approaches suffer from high-computational cost and low performance in complex multi-object scenarios, where occlusions can be present. 

- 摘要：同时进行多目标3D重建和基于单视图RGB-D的pose估计，参考CenterNet将目标表示为点。 This paper studies the complex task of simultaneous multi-object 3D reconstruction, 6D pose and size estimation from a single-view RGB-D observation. Our method treats object instances as spatial centers where each center denotes the complete shape of an object along with its 6D pose and size.

- 网络结构
    ![CenterSnap_compare](assets_pose/CenterSnap_compare.png)
    ![CenterSnap_archi](assets_pose/CenterSnap_archi.png)

<summary>
</details>


<details>
<summary> <b> TemporalFusion (IROS 2021) -- 强相关！ </b> </summary>

- 自评：1.该工作是model-based而非类别级；2. 时序融合的方式还是太粗糙，直接concat，不过好歹避免了对齐问题（FaF中直接concat特征图会引入对不齐的问题）；3. 实验方面仅仅对比了DenseFusion。

- 摘要：we present an end-to-end model named TemporalFusion, which integrates the temporal motion information from RGB-D images for 6D object pose estimation. The core of proposed TemporalFusion model is to embed and fuse the temporal motion information from multi-frame RGBD sequences, which could handle heavy occlusion in robotic grasping tasks. Furthermore, the proposed deep model can also obtain stable pose sequences, which is essential for realtime robotic grasping tasks. We evaluated the proposed method in the YCB-Video dataset.

- 网络结构：
    - (1) 语义分割得目标mask，其rgb和depth分别由PSPNet和Pointnet提特征；不同于DenseFusion，作者提出基于采样的特征融合，对于第$i$帧，采样$N*\alpha_i$个点，其中$\sum_{i=1}^t \alpha_i = 1$，于是t帧数据融合总共就有N个点，特征维度256；图中展示的效果是，对于远离当前帧的早期帧，采样点可以相对少一些；
    - (2) 运动推理：基于Open3D的视觉里程计算出位姿变化（这部分待续）；
    - (3) 时序融合：基于全局池化和最大池化，获取全局特征；用两个3-layer的卷积网络，将运动推理模块预测的$R'$和$T'$分别转化为运动特征；再结合N个逐点特征（局部），全部堆叠起来得融合的时序特征；
    - (4) 考虑到不同特征对最终的pose估计贡献不同，采用CBAM注意力给特征的通道加权，然后接3个head分别预测$R$, $T$和置信度$c$，取置信度最大的$c$对应的预测作为最终结果。
    ![TemporalFuse_archi](assets_pose/TemporalFuse_archi.png)
    ![TemporalFuse_reg](assets_pose/TemporalFuse_reg.png)

<summary>
</details>


<details>
<summary> <b> YOLOPose (arxiv 2022) 关注 </b> </summary>

- **摘要**：We propose YOLOPose, a **Transformer-based multi-object monocular 6D pose estimation** method based on **keypoint regression**. In contrast to the standard heatmaps for predicting keypoints in an image, we directly regress the keypoints.  Additionally, we employ a learnable orientation estimation module to predict the orientation from the keypoints. Our model is **end-to-end** differentiable and is suitable for **real-time** applications. ...test on the YCBVideo dataset.

- 注：32 keypoints (the eight corners of the 3D bounding box and the 24 intermediate bounding box keypoints)

- **示意图**
    ![YOLOPose_vis](assets_pose/YOLOPose_vis.png)

- **网络结构**
    ![YOLOPose_archi](assets_pose/YOLOPose_archi.png)

<summary>
</details>


<details>
<summary> <b> OnePose (CVPR 2022) 关注(SFM重建)  </b> </summary>

- OnePose：One-shot之意！
- **摘要**：OnePose draws the idea from visual localization and only requires a simple RGB video scan of the object to build a sparse SfM model of the object. We propose a new graph attention network that directly matches 2D interest points in the query image with the 3D points in the SfM model, resulting in efficient and robust pose estimation. ...run in real-time. ... test on self-collected dataset that consists of 450 sequences of 150 objects.

- 关注related works章节；摘录对NOCS系列方法的评价： A limitation of this line of work is that the shape and the appearance of some instances could vary significantly even they belong to the same category, thus the generalization capabilities of trained networks over these instances are questionable. Moreover, accurate CAD models are still required
for ground-truth NOCS map generation during training, and different networks need to be trained for different categories. 总结就是NOCS只是测试阶段不需要CAD models，训练阶段仍需要，因此在本文中仍被划分为Model-Based方法。

- **对比各种设置的示意图**
    ![OnePose_vis](assets_pose/OnePose_vis.png)

- **算法流程**
    ![OnePose_overview](assets_pose/OnePose_overview.png)


<summary>
</details>


<details>
<summary> <b> Morefusion (CVPR 2020) 关注 </b> </summary>

- 处理known objects
- **摘要**：We present a system which can estimate the accurate poses of multiple **known objects** in contact and occlusion from real-time, embodied multi-view vision. Our approach makes 3D object pose proposals from single RGBD views, accumulates pose estimates and non-parametric occupancy information from multiple views as the camera moves, and performs joint optimization to estimate consistent, non-intersecting poses for multiple objects in contact. ...test on YCB-Video, and our own challenging Cluttered YCB-Video.

- **pipiline的四个阶段**：
    - object-level volumetric fusion: 用目标实例的mask处理RGB+depth，再结合相机位姿的tracking(基于ORB-SLAM2)，创建一个volumetric map(包括已知的目标和未知的目标)；
    - volumetric pose prediction： 利用volumetric map作为目标周围的信息，结合目标经mask后的特征grid，估计一个初始的位姿；
    - collision-based pose refinement：使用物体CAD模型上的采样点(经估计的pose转换)，和volumatric map上的occupied space进行碰撞检查，通过梯度下降联合优化多个目标的位姿；
    - CAD alignment：将多个相机坐标系下估计的目标物体pose，转换到统一的世界坐标系下，然后两两计算pose loss一并优化，使各视角下预测的pose是一致的。

- **网络结构**
    ![MoreFusion_archi](assets_pose/MoreFusion_archi.png)

- **子网络结构**
    ![MoreFusion_sub](assets_pose/MoreFusion_sub.png)

<summary>
</details>


<details>
<summary> <b> Cosypose (ECCV 2020) </b> </summary>

- 处理known objects；Cosy来自consistency之意！
- **摘要**：We introduce an approach for recovering the 6D pose of multiple known objects in a scene captured by a set of input images with unknown camera viewpoints. (1) We present a **single-view single-object** 6D pose estimation method to generate pose hypotheses; (2) We **jointly estimate** camera viewpoints and 6D poses of all objects in a single consistent scene; (3) We develop a method for global scene refinement by solving an **object-level bundle adjustment** problem. ... test on YCB-Video and T-LESS datasets.

- **示意图**
    ![CosyPose_vis](assets_pose/CosyPose_vis.png)

- **算法流程**
    ![CosyPose_pipe](assets_pose/CosyPose_pipe.png)

<summary>
</details>


<details>
<summary> <b> Latentfusion (CVPR 2020) 关注 </b> </summary>

- 处理unseen objects
- **摘要**：We present a network that reconstructs a latent 3D representation of an object using a small number of reference views at inference time. Our network
is able to render the latent 3D representation from arbitrary views. Using this neural renderer, we directly optimize for pose given an input image. By training our network with a large number of 3D shapes for **reconstruction and rendering**, our network generalizes well to **unseen objects**. We present a new dataset for unseen object pose estimation–**MOPED**. ...test on MOPED
as well as the ModelNet and LINEMOD datasets.

- **网络结构**
    ![LatentFusion_archi](assets_pose/LatentFusion_archi.png)

- **子网络结构**
    ![LatentFusion_archi2](assets_pose/LatentFusion_archi2.png)

<summary>
</details>


<details>
<summary> <b> RLLG (CVPR 2020)  </b> </summary>

- **摘要**：We propose a learning-based method whose input is a collection of images of a target object, and whose output is the pose of the object in a novel view. At inference time, our method maps from the RoI features of the input image
to a dense collection of object-centric 3D coordinates, one per pixel. This dense 2D-3D mapping is then used to determine 6dof pose using standard PnP plus RANSAC. We seamlessly build our model upon Mask R-CNN. We contribute a new head – the object coordinate head – to the same backbone, whose output is the dense 3D coordinates of the object in object-centric frame. 

- **推理阶段可视化**
    ![RLLG_vis](assets_pose/RLLG_vis.png)


<summary>
</details>



<details>
<summary> <b> SGPA (ICCV 2021)  </b> </summary>

- **摘要**：We take advantage of category prior to overcome the problem of intra-class variation by innovating a structure-guided prior adaptation scheme to accurately estimate 6D pose for individual objects. We propose to leverage their structure similarity to dynamically adapt the prior to the observed object

- **网络结构**
    ![SGPA_archi](assets_pose/SGPA_archi.png)


<summary>
</details>



<details>
<summary> <b> PPR-Net (CVPR 2020)  </b> </summary>

- **摘要**：We propose a simple but novel Point-wise Pose Regression Network (PPR-Net). For each point in the point cloud, the network regresses a 6D pose of the object instance that the point belongs to. We argue that the regressed poses of points from the same object instance should be located closely in pose space. Thus, these points can be clustered into different instances and their corresponding objects’ 6D poses can be estimated simultaneously. It works well in real world robot
bin-picking tasks.

- **网络结构** 
    - (1) 输入点云，先Pointnet++提特征，后接4个分支；其中一个做语义分割，得到逐点的类别预测，将语义类别concat到原点云特征，得到所谓semantic-class-aware的特征；该组合特征输入到另外3个分支，分别回归逐点的center预测，逐点的旋转角预测，逐点的物体可见性预测（可见性衡量了该点所属物体被遮挡的程度）；
    - (2) 推理阶段：高于可见性阈值的点，才有voting的权利，先将这些点基于密度聚类（同一物体上的点，预测的center位置应该是聚在一起的），相当于在前面语义分割的基础上，再进行实例分割，然后每个实例的pose，就是它包含的有效点的voting的平均；
    - (3) 训练阶段：包含3个损失，一个是语义分割（即逐点的分类）损失，用的交叉熵；一个是逐点的可见性，这里gt是启发式得到的，用当前点所属物体包含的点数，除以场景中一个物体包含的最大点数，即近似了物体被遮挡程度；第三个是pose约束，用到了另一篇文章中定义的pose metric，直接在欧式空间中算distance！
    ![PPRNet_archi](assets_pose/PPRNet_archi.png)


<summary>
</details>


<details>
<summary> <b> ... ()  </b> </summary>

- **摘要**：

- **网络结构**
    ![](assets_pose/.png)


<summary>
</details>



